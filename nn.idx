Variable	nn.html#Variable	nn: Variable	
InitFunc	nn.html#InitFunc	nn: InitFunc	
Optimizer	nn.html#Optimizer	nn: Optimizer	
SGDOptimizer	nn.html#SGDOptimizer	nn: SGDOptimizer	
AdamOptimizer	nn.html#AdamOptimizer	nn: AdamOptimizer	
Scheduler	nn.html#Scheduler	nn: Scheduler	
StepLR	nn.html#StepLR	nn: StepLR	
CosineAnnealingLR	nn.html#CosineAnnealingLR	nn: CosineAnnealingLR	
Outputs	nn.html#Outputs	nn: Outputs	
Module	nn.html#Module	nn: Module	
initRandom	nn.html#initRandom,int64	nn: initRandom(seed: int64 = 0): Rand	
constantInit	nn.html#constantInit,SomeFloat	nn: constantInit(value: SomeFloat): InitFunc	
uniformInit	nn.html#uniformInit,float,float	nn: uniformInit(min = 0.0; max = 1.0): InitFunc	
normalInit	nn.html#normalInit,float,float	nn: normalInit(mean = 0.0; stddev = 1.0): InitFunc	
glorotInit	nn.html#glorotInit,float	nn: glorotInit(gain = 1.0): InitFunc	
heInit	nn.html#heInit,float,float	nn: heInit(mean = 0.0; gain = 2.0; fanOut = false): InitFunc	
newVariable	nn.html#newVariable,Client,string,openArray[int],InitFunc,Rand	nn: newVariable(c: Client; name: string; dims: openArray[int]; init: InitFunc;\n            rng: var Rand; calcGrad = true): Variable	
param	nn.html#param,Builder,Variable,string	nn: param(b: Builder; p: Variable; suffix = &quot;&quot;): Node	
add	nn.html#add,Module,varargs[Module]	nn: add(m: var Module; modules: varargs[Module])	
`$`	nn.html#$,Module	nn: `$`(m: Module): string	
setVars	nn.html#setVars,Module,varargs[Variable]	nn: setVars(m: var Module; vars: varargs[Variable])	
learnableVars	nn.html#learnableVars,Module	nn: learnableVars(m: Module): seq[Variable]	
varNames	nn.html#varNames,Module	nn: varNames(m: Module): seq[string]	
gradNames	nn.html#gradNames,Module	nn: gradNames(m: Module): seq[string]	
getParams	nn.html#getParams,Module,Params	nn: getParams(m: Module; params: var Params)	
setParams	nn.html#setParams,Module,Params	nn: setParams(m: var Module; params: Params)	
update	nn.html#update,Module,Params	nn: update(m: var Module; params: Params)	
mseLoss	nn.html#mseLoss,Node,Node	nn: mseLoss(pred, target: Node): Node	
crossEntropyLoss	nn.html#crossEntropyLoss,Node,Node	nn: crossEntropyLoss(pred, target: Node): Node	
softmax	nn.html#softmax,Node,int	nn: softmax(a: Node; axis = -1): Node	
dropout	nn.html#dropout,Node,float,bool	nn: dropout(a: Node; ratio: float; training: bool; normalize = true): Node	
initLinear	nn.html#initLinear,Client,Rand,string,int,int	nn: initLinear(c: Client; rng: var Rand; id: string; nin, nout: int;\n           weights = heInit(); biases = constantInit(0.0); dtype = F32): Module	
initConv2d	nn.html#initConv2d,Client,Rand,string,int,int,Opt2d,Opt2d,Pad2d,Opt2d,int	nn: initConv2d(c: Client; rng: var Rand; id: string; inChannels, outChannels: int;\n           kernelSize: Opt2d; strides: Opt2d = 1; padding: Pad2d = pad(0);\n           dilation: Opt2d = 1; groups = 1; weights = heInit();\n           biases = constantInit(0.0); dtype = F32): Module	
initBatchNorm	nn.html#initBatchNorm,Client,Rand,string,int,float,float	nn: initBatchNorm(c: Client; rng: var Rand; id: string; numFeatures: int;\n              momentum: float = 0.1; epsilon: float = 0.00001;\n              weights = constantInit(1.0); biases = constantInit(0.0);\n              dtype = F32): Module	
compileTest	nn.html#compileTest,Client,Module,Node	nn: compileTest(c: Client; m: Module; input: Node): Executable	
compileTrain	nn.html#compileTrain,Client,Module,Node,proc(Node)	nn: compileTrain(c: Client; m: Module; input: Node; lossFn: proc (y: Node): Node): Executable	
format	nn.html#format,float	nn: format(val: float): string	
learningRate	nn.html#learningRate,Optimizer	nn: learningRate(optim: Optimizer): float	
setLearningRate	nn.html#setLearningRate,Optimizer,Client,float	nn: setLearningRate(optim: var Optimizer; c: Client; lr: float)	
step	nn.html#step,Optimizer,Params	nn: step(optim: var Optimizer; params: Params): Params	
`$`	nn.html#$.e,Optimizer	nn: `$`(optim: Optimizer): string	
optimSGD	nn.html#optimSGD,Client,Module,float,float,float	nn: optimSGD(c: Client; m: Module; learnRate: float; weightDecay = 0.0;\n         momentum = 0.0; nesterov = false): Optimizer	
`$`	nn.html#$.e,SGDOptimizer	nn: `$`(optim: SGDOptimizer): string	
optimAdam	nn.html#optimAdam,Client,Module,float,float,float,float,float	nn: optimAdam(c: Client; m: Module; learnRate: float; weightDecay = 0.0;\n          beta1 = 0.9; beta2 = 0.999; eps = 1e-8): Optimizer	
optimAdamW	nn.html#optimAdamW,Client,Module,float,float,float,float,float	nn: optimAdamW(c: Client; m: Module; learnRate: float; weightDecay = 0.0;\n           beta1 = 0.9; beta2 = 0.999; eps = 1e-8): Optimizer	
`$`	nn.html#$.e,AdamOptimizer	nn: `$`(optim: AdamOptimizer): string	
`$`	nn.html#$.e,Scheduler	nn: `$`(s: Scheduler): string	
init	nn.html#init,Scheduler,Client,int	nn: init(s: Scheduler; c: Client; epoch: int)	
step	nn.html#step,Scheduler,Client	nn: step(s: Scheduler; c: Client)	
newStepLR	nn.html#newStepLR,Optimizer,int,float	nn: newStepLR(optim: var Optimizer; stepSize: int; gamma = 0.1): StepLR	
`$`	nn.html#$.e,StepLR	nn: `$`(s: StepLR): string	
newCosineAnnealingLR	nn.html#newCosineAnnealingLR,Optimizer,int,float	nn: newCosineAnnealingLR(optim: var Optimizer; tMax: int; lrMin = 0.0): CosineAnnealingLR	
`$`	nn.html#$.e,CosineAnnealingLR	nn: `$`(s: CosineAnnealingLR): string	
